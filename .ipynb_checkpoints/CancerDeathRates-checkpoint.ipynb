{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **FINAL PROJECT REPORT** - **GROUP 8**\n",
    "# SUBJECT: DATA MINING\n",
    "# *TOPIC: PREDICTING CANCER MORTALITY IN U.S. COUNTIES*\n",
    "# List of members:\n",
    "1. Duong Van Nhat Long - 20521561\n",
    "2. Vo Đoan To Loan - 20521544\n",
    "3. Nguyen Thanh Luan - 20521582\n",
    "4. Dinh Thi Tu Uyen - 20522139\n",
    "\n",
    "### About dataset: These data were aggregated from a number of sources including the American Community Survey (census.gov), clinicaltrials.gov, and cancer.gov and this dataset have 34 feature and 3047 samples\n",
    "### Data Source: https://data.world/nrippner/ols-regression-challenge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import to libraries that manipulate datasets and numbers: numpy, pandas, graphing libraries, data visualization: seaborn, matplotlib.\n",
    "%matplotlib inline\n",
    "import pandas as pd \n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "\n",
    "# To use machine learning algorithms, we import the sklearn library\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import tree\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn import metrics\n",
    "\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler, PolynomialFeatures\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "from sklearn.impute import KNNImputer\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "import seaborn as sns\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import dataset \"cancer_reg.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "data = pd.read_csv(\"cancer_reg.csv\", encoding='latin-1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count number of fearutes and samples\n",
    "print(\"Number of features:\", data.shape[1])\n",
    "print(\"Number of samples :\", data.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Survey dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This dataset have 34 features and 3047 samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# View basic information\n",
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Statistical statistics of quantitative attributes such as: count the number of values, maximum, minimum, mean, standard deviation, quartiles...\n",
    "data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print out each data type of each features\n",
    "dtype = pd.DataFrame(data= {'types': data.dtypes})\n",
    "dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count data type\n",
    "dtype.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see there are 29 features of type float64, 3 features of type int64, 2 features of type object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if dataset have missing values\n",
    "data.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see there are 3 features PctSomeCol18_24, PctEmployed16_Over, PctPrivateCoverageAlone have missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate percent of missing values and sample\n",
    "missing_samples = data.isna().any(axis=1).sum()\n",
    "percent_missing = (missing_samples / len(data)) * 100\n",
    "\n",
    "print('Number missing samples:', missing_samples)\n",
    "print('Percent missing samples:', percent_missing)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see 2456 samples with missing data and with 80.6%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate percentage of missing values ​​for each features and total missing values ​​for all features\n",
    "missing_data = pd.DataFrame({\n",
    "    'Number missing': data.isna().sum(),\n",
    "    'Percent %': (data.isna().sum() * 100 / len(data))\n",
    "})\n",
    "missing_data.sort_values(by='Percent %', ascending=False, inplace=True)\n",
    "\n",
    "missing_data_total = pd.DataFrame({\n",
    "    'Number missing': data.isna().sum(),\n",
    "    'Percent %': (data.isna().sum() * 100 / data.size)\n",
    "})\n",
    "\n",
    "print(missing_data_total.sum())\n",
    "print(missing_data.head(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that there are 3046 missing values ​​and account for about 2.94% of the total values\n",
    "+ The PctSomeCol18_24 property has 2285 and is about 75%\n",
    "+ The PctPrivateCoverageAlone property has 609 and is about 19%\n",
    "+ The PctEmployed16_Over property has 152 and is about 5%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 10))\n",
    "sns.barplot(x=missing_data.head(3).index, y='Percent %', data=missing_data.head(3))\n",
    "\n",
    "plt.xticks(rotation='horizontal')\n",
    "plt.xlabel('Features')\n",
    "plt.ylabel('Percent')\n",
    "plt.title('Percent Missing')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that PctSomeCol18_24 has quite a large data loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DATA PREPROCESSING"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because PctSomeCol18_24 has too much data loss (75%), we will remove it before entering the algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.drop('PctSomeCol18_24', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FEATURE ENGINEERING"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### *Feature Creation*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract County and State from Geography\n",
    "County = []\n",
    "State = []\n",
    "for i in range(len(data)):\n",
    "  County.append(data['Geography'][i][0:data['Geography'][i].find(','):])\n",
    "  State.append(data['Geography'][i][data['Geography'][i].find(',') + 2::])\n",
    "\n",
    "data['County'] = County\n",
    "data['State'] = State"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# View columns after splitting\n",
    "data[['Geography','County','State']].head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### We need to encoder ['County', 'State'] before give them to sklearn model. But 'County' have high Cardinality so that we should not encoder it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoder 'State' by Mean 'TARGET_deathRate'\n",
    "## Create dictionary to mapping encoder\n",
    "dic_map = data[['State', 'TARGET_deathRate']].groupby(by= ['State']).mean().to_dict()['TARGET_deathRate']\n",
    "dic_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's replace values in 'State'\n",
    "data['State_encode'] = data['State'].map(dic_map)\n",
    "data[['State' ,'State_encode']].sample(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### *Missing Value Handling*\n",
    "##### After drop PctSomeCol18_24 , we have two features with missing data: PctPrivateCoverageAlone, PctEmployed16_Over . Here we will consider handling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Before we find the best solution, we will evaluate on the dataset copy\n",
    "data_copy = data.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_t = data.copy()\n",
    "lst = []\n",
    "sc = 0\n",
    "for i in ['mean', 'median', 'most_frequent', 'constant']:\n",
    "\n",
    "  X_o = data[['PctPrivateCoverageAlone','PctEmployed16_Over']]\n",
    "  y_o = data['TARGET_deathRate']\n",
    "\n",
    "  imputer_ = SimpleImputer(missing_values=np.nan, strategy=i)\n",
    "  X_o = pd.DataFrame(imputer_.fit_transform(X_o),columns= X_o.columns)\n",
    "\n",
    "  X_o_train, X_o_test, y_o_train, y_o_test = train_test_split(X_o, y_o, train_size= 0.85)\n",
    "\n",
    "  ln_2 = LinearRegression()\n",
    "  ln_2.fit(X_o_train,y_o_train)\n",
    "\n",
    "  lst.append(ln_2.score(X_o_test,y_o_test))\n",
    "\n",
    "  print('Score with '+i+ ' data:', ln_2.score(X_o_test,y_o_test))\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_t[['PctPrivateCoverageAlone','PctEmployed16_Over']], X_t.TARGET_deathRate, train_size= 0.85)\n",
    "ln = LinearRegression()\n",
    "ln.fit(X_train,y_train)\n",
    "\n",
    "sc = ln.score(X_test,y_test)\n",
    "lst.append(ln_2.score(X_o_test,y_o_test))\n",
    "print('Score with mean by State data:', ln.score(X_test,y_test))\n",
    "\n",
    "X = data[['PctPrivateCoverageAlone','PctEmployed16_Over']]\n",
    "y = data['TARGET_deathRate']\n",
    "imputer = KNNImputer(n_neighbors=2, weights=\"distance\")\n",
    "X = pd.DataFrame(imputer.fit_transform(X), columns= X.columns)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, train_size= 0.85)\n",
    "\n",
    "ln = LinearRegression()\n",
    "ln.fit(X_train,y_train)\n",
    "\n",
    "print('Score with KNNImputer data:', ln.score(X_test,y_test))\n",
    "\n",
    "lst.append(ln.score(X_test,y_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dictionary to mapping missing values in PctPrivateCoverageAlone by mean group by State\n",
    "dic_map = data_copy[['State', 'PctPrivateCoverageAlone']].groupby(by= ['State']).mean().to_dict()['PctPrivateCoverageAlone']\n",
    "dic_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's fill missing values in PctPrivateCoverageAlone\n",
    "data_copy['PctPrivateCoverageAlone'] = data_copy.PctPrivateCoverageAlone.fillna(data_copy.State.map(dic_map))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dictionary to mapping missing values in PctEmployed16_Over by mean group by State\n",
    "dic_map = data_copy[['State', 'PctEmployed16_Over']].groupby(by= ['State']).mean().to_dict()['PctEmployed16_Over']\n",
    "dic_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's fill missing values in PctPrivateCoverageAlone\n",
    "data_copy['PctEmployed16_Over'] = data_copy.PctEmployed16_Over.fillna(data_copy.State.map(dic_map))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now calculate percentage of missing values ​​for each features and total missing values ​​for all features\n",
    "missing_data = pd.DataFrame({\n",
    "    'Number missing': data_copy.isna().sum(),\n",
    "    'Percent %': (data_copy.isna().sum() * 100 / len(data_copy))\n",
    "})\n",
    "missing_data.sort_values(by='Percent %', ascending=False, inplace=True)\n",
    "\n",
    "missing_data_total = pd.DataFrame({\n",
    "    'Number missing': data_copy.isna().sum(),\n",
    "    'Percent %': (data_copy.isna().sum() * 100 / data_copy.size)\n",
    "})\n",
    "\n",
    "print(missing_data_total.sum())\n",
    "print(missing_data.head(2))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
